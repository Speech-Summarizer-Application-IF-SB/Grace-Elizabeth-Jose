# -*- coding: utf-8 -*-
"""SpeechSummarizer_Application.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VbdIEZP83VKNja8c9pOUVELQLWMse2f5
"""

!nvidia-smi

# Step 0: Install necessary packages
!pip install transformers datasets torch soundfile librosa

# Step 1: Import libraries
from transformers import pipeline
from google.colab import files

# Step 2: Upload your audio file
print("Upload a WAV audio file (short meeting clip recommended)")
uploaded = files.upload()  # Select audio.wav from your computer
audio_file = list(uploaded.keys())[0]

# Step 3: Initialize Whisper STT model
print("Loading Whisper model...")
stt = pipeline("automatic-speech-recognition", model="openai/whisper-small")

# Step 4: Transcribe the audio
print("Transcribing audio...")
transcription_result = stt(audio_file)
transcription_text = transcription_result["text"]
print("\n--- Transcription ---")
print(transcription_text)

# Step 5: Initialize BART summarization model
print("\nLoading BART summarization model...")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Step 6: Generate summary
print("Generating summary...")
summary_result = summarizer(transcription_text, max_length=100, min_length=30, do_sample=False)
summary_text = summary_result[0]["summary_text"]

print("\n--- Summary ---")
print(summary_text)

!pip install transformers torch soundfile librosa noisereduce --quiet

# --- Continue after audio upload ---

# Import required libraries
import librosa
import soundfile as sf
import noisereduce as nr
from transformers import pipeline
from IPython.display import Audio, display, Markdown
from google.colab import files

# Step 1: Optional - Play uploaded audio
display(Markdown("### Original Uploaded Audio"))
display(Audio(audio_file))

# Step 2: Noise reduction and resample to 16kHz
y, sr = librosa.load(audio_file, sr=16000)
cleaned_audio = nr.reduce_noise(y=y, sr=sr)
cleaned_file = "cleaned_audio.wav"
sf.write(cleaned_file, cleaned_audio, sr)

# Play cleaned audio
display(Markdown("### Cleaned Audio"))
display(Audio(cleaned_file))

# Step 3: Transcribe using Whisper
stt = pipeline("automatic-speech-recognition", model="openai/whisper-small")
transcription_result = stt(cleaned_file)
transcription_text = transcription_result["text"]
display(Markdown(f"**Transcription:** {transcription_text}"))

# Step 4: Summarize transcription using BART
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary_result = summarizer(transcription_text, max_length=100, min_length=30, do_sample=False)
summary_text = summary_result[0]["summary_text"]
display(Markdown(f"**Summary:** {summary_text}"))

# Step 5: Save transcription and summary, then download
with open("transcription.txt", "w") as f:
    f.write(transcription_text)
with open("summary.txt", "w") as f:
    f.write(summary_text)

# Save transcription and summary without automatic download
with open("transcription.txt", "w") as f:
    f.write(transcription_text)

with open("summary.txt", "w") as f:
    f.write(summary_text)

print("Transcription and summary saved as 'transcription.txt' and 'summary.txt' in Colab.")